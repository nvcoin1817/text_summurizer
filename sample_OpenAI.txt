Every year, OpenAI’s employees vote on when they believe artificial general intelligence, or AGI, will finally arrive. 
It’s mostly seen as a fun way to bond, and their estimates differ widely. 
But in a field that still debates whether human-like autonomous systems are even possible, half the lab bets it is likely to happen within 15 years.
In the four short years of its existence, OpenAI has become one of the leading AI research labs in the world. 
It has made a name for itself producing consistently headline-grabbing research, alongside other AI heavyweights like Alphabet’s DeepMind. 
It is also a darling in Silicon Valley, counting Elon Musk and legendary investor Sam Altman among its founders.
Above all, it is lionized for its mission. 
Its goal is to be the first to create AGI—a machine with the learning and reasoning powers of a human mind. 
The purpose is not world domination; rather, the lab wants to ensure that the technology is developed safely and its benefits distributed evenly to the world.
The implication is that AGI could easily run amok if the technology’s development is left to follow the path of least resistance. 
Narrow intelligence, the kind of clumsy AI that surrounds us today, has already served as an example. 
We now know that algorithms are biased and fragile; they can perpetrate great abuse and great deception; and the expense of developing and running them tends to concentrate their power in the hands of a few. 
By extrapolation, AGI could be catastrophic without the careful guidance of a benevolent shepherd.
OpenAI wants to be that shepherd, and it has carefully crafted its image to fit the bill. 
In a field dominated by wealthy corporations, it was founded as a nonprofit. 
Its first announcement said that this distinction would allow it to “build value for everyone rather than shareholders.” 
Its charter—a document so sacred that employees’ pay is tied to how well they adhere to it—further declares that OpenAI’s “primary fiduciary duty is to humanity.” 
Attaining AGI safely is so important, it continues, that if another organization were close to getting there first, OpenAI would stop competing with it and collaborate instead. 
This alluring narrative plays well with investors and the media, and in July Microsoft injected the lab with a fresh $1 billion.
But three days at OpenAI’s office—and nearly three dozen interviews with past and current employees, collaborators, friends, and other experts in the field—suggest a different picture. 
There is a misalignment between what the company publicly espouses and how it operates behind closed doors. 
Over time, it has allowed a fierce competitiveness and mounting pressure for ever more funding to erode its founding ideals of transparency, openness, and collaboration. 
Many who work or worked for the company insisted on anonymity because they were not authorized to speak or feared retaliation. 
Their accounts suggest that OpenAI, for all its noble aspirations, is obsessed with maintaining secrecy, protecting its image, and retaining the loyalty of its employees.